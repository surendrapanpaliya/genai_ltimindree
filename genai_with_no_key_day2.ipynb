{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef0482ac",
   "metadata": {},
   "source": [
    "pip install --upgrade openai python-dotenv\n",
    "•\topenai is the official Python client.  \n",
    "•\tpython-dotenv helps load OPENAI_API_KEY from a .env file (cleaner than hard-coding)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7636b7",
   "metadata": {},
   "source": [
    "Create .env file\n",
    "\n",
    "OPENAI_API_KEY=\"Your API KEY\"\n",
    "\n",
    "2) Set your API key (safely)\n",
    "Create a .env file (same folder as your script):\n",
    "OPENAI_API_KEY=sk-...your key...\n",
    "Then load it in Python (next step). OpenAI recommends environment variables for key safety.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91f3d3a",
   "metadata": {},
   "source": [
    "3) Initialize the client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd2ed2d",
   "metadata": {},
   "source": [
    "4) Make your first chat call (GPT-5)\n",
    "\n",
    "Use the model name your account is provisioned \n",
    "\n",
    "for (e.g., \"gpt-5\" or a specific snapshot you see in the dashboard).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4911c051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teams adopt feature flags in CI/CD to:\n",
      "\n",
      "- Decouple deploy from release: ship code “dark,” turn it on later without redeploying.\n",
      "- Enable progressive delivery: cohort/percentage/region rollouts, canaries, rings; instant kill switch/rollback.\n",
      "- Support trunk-based development: hide incomplete work, reduce long-lived branches and merge risk.\n",
      "- Validate safely in production: internal/beta access, dark launches, shadow traffic; gate risky DB/API migrations (e.g., dual-read/dual-write).\n",
      "- Run experiments: A/B tests, personalization, entitlement-based access; measure impact before full rollout.\n",
      "- Improve operability: toggle behavior during incidents, throttle features, control cost-heavy paths.\n",
      "- Cut lead time and MTTR: smaller changes, fewer emergency deploys to undo mistakes.\n",
      "- Coordinate multi-service changes: avoid lockstep releases across dependencies.\n",
      "- Manage per-env/tenant/region differences: meet compliance or customer-specific needs.\n",
      "- Enhance observability: correlate flags with metrics to make data-driven release decisions.\n",
      "\n",
      "Note: Flags add complexity—use ownership, clear defaults, expirations, audit/metrics, and cleanup to manage flag debt.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # pulls OPENAI_API_KEY from .env\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "resp = client.responses.create(\n",
    "    model=\"gpt-5\",  # replace with your provisioned GPT-5 model\n",
    "    input=\"You are a concise senior developer assistant.Summarize why teams adopt feature flags in CI/CD\")\n",
    "\n",
    "print(resp.output_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422fa0da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Under a silver moon, a sleepy unicorn tiptoed through a whispering meadow, sprinkling stardust over every child’s dreams until the whole world sighed goodnight.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # pulls OPENAI_API_KEY from .env\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-5\",\n",
    "    input=\"Write a one-sentence bedtime story about a unicorn.\"\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f7499f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "#!/usr/bin/env python3\n",
      "\"\"\"\n",
      "Prime number utilities: primality test and prime generation.\n",
      "\n",
      "Functions:\n",
      "- is_prime(n): True if n is prime.\n",
      "- primes_up_to(n): List primes <= n via Sieve of Eratosthenes.\n",
      "\n",
      "CLI:\n",
      "  Check a single number:\n",
      "    python prime_utils.py 97\n",
      "\n",
      "  List primes up to N:\n",
      "    python prime_utils.py --up-to 100\n",
      "\"\"\"\n",
      "\n",
      "from __future__ import annotations\n",
      "\n",
      "import argparse\n",
      "import math\n",
      "from typing import List\n",
      "\n",
      "\n",
      "def is_prime(n: int) -> bool:\n",
      "    \"\"\"\n",
      "    Determine if n is a prime number.\n",
      "\n",
      "    Args:\n",
      "        n: Integer to test.\n",
      "\n",
      "    Returns:\n",
      "        True if prime, False otherwise.\n",
      "    \"\"\"\n",
      "    if n <= 1:\n",
      "        return False\n",
      "    if n <= 3:\n",
      "        return True  # 2, 3\n",
      "    if n % 2 == 0 or n % 3 == 0:\n",
      "        return False\n",
      "\n",
      "    limit = math.isqrt(n)\n",
      "    i = 5\n",
      "    while i <= limit:\n",
      "        if n % i == 0 or n % (i + 2) == 0:\n",
      "            return False\n",
      "        i += 6\n",
      "    return True\n",
      "\n",
      "\n",
      "def primes_up_to(n: int) -> List[int]:\n",
      "    \"\"\"\n",
      "    Generate all primes p with 2 <= p <= n.\n",
      "\n",
      "    Uses the Sieve of Eratosthenes.\n",
      "\n",
      "    Args:\n",
      "        n: Inclusive upper bound.\n",
      "\n",
      "    Returns:\n",
      "        List of primes up to n.\n",
      "    \"\"\"\n",
      "    if n < 2:\n",
      "        return []\n",
      "    sieve = bytearray(b\"\\x01\") * (n + 1)\n",
      "    sieve[0:2] = b\"\\x00\\x00\"\n",
      "\n",
      "    limit = math.isqrt(n)\n",
      "    for p in range(2, limit + 1):\n",
      "        if sieve[p]:\n",
      "            start = p * p\n",
      "            sieve[start : n + 1 : p] = b\"\\x00\" * (((n - start) // p) + 1)\n",
      "\n",
      "    return [i for i in range(2, n + 1) if sieve[i]]\n",
      "\n",
      "\n",
      "def _build_parser() -> argparse.ArgumentParser:\n",
      "    parser = argparse.ArgumentParser(\n",
      "        description=\"Prime number utilities: check primality or list primes.\"\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \"n\",\n",
      "        type=int,\n",
      "        nargs=\"?\",\n",
      "        help=\"Number to test for primality (omit when using --up-to).\",\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \"--up-to\",\n",
      "        type=int,\n",
      "        metavar=\"N\",\n",
      "        help=\"List all primes up to N (inclusive).\",\n",
      "    )\n",
      "    return parser\n",
      "\n",
      "\n",
      "def main() -> int:\n",
      "    parser = _build_parser()\n",
      "    args = parser.parse_args()\n",
      "\n",
      "    if args.up_to is not None:\n",
      "        print(primes_up_to(args.up_to))\n",
      "        return 0\n",
      "\n",
      "    if args.n is None:\n",
      "        parser.error(\"Provide a number to check or use --up-to N.\")\n",
      "        return 2\n",
      "\n",
      "    print(f\"{args.n} is {'a prime' if is_prime(args.n) else 'not a prime'}.\")\n",
      "    return 0\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    raise SystemExit(main())\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.responses.create(\n",
    "  model=\"gpt-5\",\n",
    "  input=[\n",
    "    {\n",
    "      \"role\": \"developer\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": \"input_text\",\n",
    "          \"text\": \"You are Python Developer. Write python code with document. Folllow best practices for generating any code. \"\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": \"input_text\",\n",
    "          \"text\": \"write a program for prime number.\"\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": \"input_text\",\n",
    "          \"text\": \"write a program for prime number.\"\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"type\": \"reasoning\",\n",
    "      \"id\": \"rs_68bf16a8eedc819daa9a9ec79175303a0194eb11b43209d6\",\n",
    "      \"summary\": [\n",
    "        {\n",
    "          \"type\": \"summary_text\",\n",
    "          \"text\": \"**Planning prime number program**\\n\\nI need to write Python code with proper documentation and best practices. The user repeated their request, which suggests they want a program that can either check if a number is prime or generate a list of primes up to a certain number, n. \\n\\nI could create a small command-line tool that covers both functions. I'll include docstrings, type hints, tests, and implement an efficient primality test using trial division with optimizations. I also plan to include functions like is_prime and primes_up_to. Let’s get started!\"\n",
    "        },\n",
    "        {\n",
    "          \"type\": \"summary_text\",\n",
    "          \"text\": \"**Designing the CLI tool**\\n\\nI'm setting up a command-line interface (CLI) using argparse. There will be two modes: \\\"check\\\" to determine if an integer is prime, and \\\"list\\\" to return all primes up to a given number N. I might also include a \\\"range\\\" option for specifying start and end limits, while striving to keep it minimal. \\n\\nThe program will consist of the `is_prime` function for primality testing and the `primes_up_to` function using the sieve of Eratosthenes. I’ll also ensure to include docstrings for clarity and follow best practices for documentation and tests. Let’s create this tool!\"\n",
    "        },\n",
    "        {\n",
    "          \"type\": \"summary_text\",\n",
    "          \"text\": \"**Finalizing code details**\\n\\nI'm working on crafting the module with a clear docstring that describes its functionality. While using logging is an option, it doesn't seem necessary for this project. I’ll include doctests within the `is_prime` function’s docstring and provide a main function to parse arguments.\\n\\nFor performance, I could implement the Miller-Rabin deterministic test for 64-bit numbers, but the 6k±1 optimization will suffice for typical use. I'll note the performance considerations for large values in the docstring and keep the formatting simple, focusing on clear, effective code. Let’s get started!\"\n",
    "        }\n",
    "      ],\n",
    "      \"encrypted_content\": \"gAAAAABovxa8RtNNszZjoaXyuTlKx6UJ6E_Gh0ZXv_G63HsZgXMsbTvhOab_JXn7a4ekSiPWkAiLxNJ-yx7d5SLzJuyFzZJEWqI4CBFsXBYpA_omWRhq_r7zbqvnY1MOZY257OSIGbgMS6mXv_il2CppcGPvl5l9DXT-gJ2xf72HMCgwDFaHbmmqNLEpK6HRegYAbnTQc53iavTHqzoeqishQpuPkdiSdRf9Lu0N3l9c7pz_Oa95PRBpugCzi_IC65gGLGNHQcQZWV_YHhSy7r4v05NCuZoh-1o7J3OH_0kos9ONZyb1sEI_ydBSAVa_Kzy4LJrHzGbxiXbyKDY5YVQnqiK5VH8fVEaTfFfs0RsCsseq-82SARtoV-jk2ziQCFGFJfOJq2lHI8i4EHk_Y0Oi1eqQNxpJ6Zjf1uqq93-mzsaQh_DPP8B8t_erFYOxv-tYB5YLFa_dcoY2vL2UWq5OzWXSXP-vYAvIVz_dUNDVi3fEc1dnnTWkAwAnZI7zF26xnaUIx4v5tk9Pn7MIAWSV9_oF6fJWzL6zBFb325XQfrSxIzFK6KTfnU7qXz2yKRHlZRNS0BdgzAUCxUyEPkMKrQMwDpmj9uiN2krZvUYQSFcuiWoL-g8ku9Unwa6tfcg6gKrWcWEcAU4lmLj5hdOT899_WsQxXHnWM9TCVFdzcPLA6mXcilKurD8iyd9p7VwPiK0Jh1glbOnnq7-we6oQ5vksysmuQfa59TDgmmfC3pHBRh_fn0x9dqNZ-dvxqs9UZQu4QArg5BYTGvfYJHgwVoYGAJ4DRghnViGUQ3ctKLUnT4EuftsWBq9ThqnmTYVD71t18GzMo934lnOCNu1dRB5U90cXwU8U1qozgm5gKHBolUGAAmY4d4NaIDF5m-xxOwnAdgZu4XAh514PI0K0sQ2f200tcezz1x2pzRWP-uJ4KiIjCO0cpcQjLjvwziOlWWI_3E7ObpghHVZNfdlo7eCdlOS9lxgbEFbwrLub0l7SINPg9IafzjVn7tzmbnQzuaJiBAc0iKI1H5o6v9OwKhT3v_0GsVOoBDfykcRtasXaK0yO-BqNWsFIccz441H4E2IUUqbQANB5DIq7mu7bPY8pXLKEwE7fjvgU0TlHGQGrSwjqooUTVF7UmBrHe78EnEHB69yttvu_b1qVSB1u5rbLTNa63umnkxTilA6BVnPeUxQYewYoZ9b1LFPIR97RRQvU_OOCHsuMmFcOaGIhsLAIKN-j86ahkSf6glCk-clxYEptEYtiGUNNxlKd44aWt0A2NRRPwOINshm12-aMGUzbLowlz25qI6YRv_LXZCM1vunEpyySk4v7XOe4ScH2kYFKwqC2ozsMHZGAQzRG78I2_3SqecbbroXwcGS0h7jhXcl4YjKXgLlAqcXBviEc_twz87QNew1WoscSVPeyuwGNog8qCuU4xuQn0HzjHeCrrUgisPm-jK32cebfmeavma1KybVRgbra0UBO2ulcWFT-R-9pnyqMGFIgCe8Hv3-kW9gIpzGTKOyFryZzb_XNF62Ks5mH6guomKSjtdMlZ4hQnD57O07TPzSn8T4gEhazBfOhbD6l6VrZDHDLDXV0MGrhqbXt8v3YR9DiDA6Lti81r0Bs8rDVzeCVQBU5WBEb3HVmEgNtX_Ynm5pRuebvliLSNIF3JIj98d9cuZUIvrYU_FrFraZIqbmVcyCU7xWXcJ860b3GjxjZQoNIifynN_0cK9plnkz5ChTerAwsPkM10N9XLv33s_5KfZh_bLCbURPJUv31z_I4_W2NwO1jw-fo1Gke31jTQ9oH7-2kubC6MDMzMUmO9467WwdtYwYSvbldRsrq1NlBJ-r57EEDA1CHPX5tH4J7-XwciNwscBqAw1dC5WTtuD8RP3G11bXSssHYDuwRjTCGBjjJ7AKaT3WqK80V-nYszcNXi3dfii6zudxZ3orhPu6u6QFIsKpuBdVfQhT4e-3KDMKrA1uxSvOzDWUJ3tPAlyRyLWFMNFEuBHUndO49dBKYEIaEoWpcBLu266HlLABIhu45UzpUaKytu81HaH6jQx5zLieqZyJwSVVKf3rRVKBVnGkwpj4pFBxHAKDU_Dr92edxIQ3siuaeNK91hzHdmuuz6MYfUDjnUDKIzRNpknZxupOjz94o2BSvZlQmTPjCcT5GoK-FO-1Gv_mmojCckIdhkZq9NoSh6al0CLpvHFHFU3aZsEIF1OP4uAOzovu6f9N0KtOAM3ekHYEbBcqi5wjIO6reyrXzU6QSmsSyT23PE5I7mBP0xOZqcMH3LU71CqMlAG8DCMF7aV63xBUjhdmcKqAd6hvsIWQmHCX46PkKRmzkOqXlSzm5_jK9jAHy1Odbwh5QzgvMSbFNYSIVnt6-DkzzxeQwMe96AiRHZVnej-EWuLABRuBreWzneCTvsC4M8A7PWcKTbrVs523FYJXxw1Rt--G2zwQ5k0iLzBZU8M21M5ElH2-iIFCbLDi4yx8uILrd9OJWSfoLVl8ye2ZoPzIRXWm4IFHuHLqvqM8mTKQAqevDE5n8JDGfw_V6ajzYa39tiDq602pAy611GYYUc1fqK16X-CAtg3P7uvYLg3Hh-AE2urdEFcKb_ewSMUQ3im1UX2Tos4xuA6UFAQ4vEUHwz0LM71d8jvW74-_8p6-YCIu308_OzPwnpWe8uR-vI1GDLn1ObvsC9xiYBcqPsrcXc5tfPZNZZ_7eok0hlLon82v78MAq-7PvGw2L2OTl8v_dHBwNW2vSOd53hkw_Mb-UdMQWRcYL_XO4ozDgQ6Vhqdz574vCIWTw-Q7U8ueyzGrVm71HgpO0QnRNgfo8edVNDvhRQbb0TFSY-QIuHnFLvPJDL-Uu5yMTq1uoclloBRWhuy7Boch7UFqlkrssjamLta8hpJDWZgYA-JGJzgmmIn19sLziSA8ppq4SZyg4HjYU-YT6ORl-RZKj8kcZA1uJLGi5kFVO02ClZgf1pOMa1MycIwxh4ZJ9n4A63cp19jF3ISuL2UofeGE13BkpTv4CPkO-zqiOz_-JtfvIgHqSxqEvyaaWGJ_bmiTjopB6oQivX-BC69QcXwDg3lRJ8V5h5XnNSAuUCAcZWv4N9O1h2kplmOxxZ3p2RqdKyd8oPLkAYTiZpuOJFdE784AnkwyqouMfATkWhxm7eDlRKnbmdjuLY3RmtQ-Oqfdm2aGopTTWz6wbYcl24sc2wCnvTfLC0yww3SM3UFmDeDlnGKFZYTh_zom8sBayo236jdwAX5DXfMrvTzOPDTv1YBL0cndVOmgJQkYNc5DMtoBqRzgaOQMz5vxDa5nX-cHLpEEKPkqjRjam5vAIeX54Zsv1-CzQkfxPJkAwTOsnpD0iKkmjFiC3PN0rTjNUKxVx0UpY5LUup-VOzhoXgJlgjmb7r6IJnk43RAn8C8AfEIoQF2Z3gw-CdbctsXvlKzAGHOs_2y8rp3J7WbRsAMGng7y9medeCt3AoKzxbg7Q4g0NJ5N3Y3qvZI1gn1P7kRSqdOPaV8rY0eefnUPD8H5lPXaMuuxgNJVaES1b30D9ab8IPKJrjHhIp5gKIEOm3dcdjtx44aT244dZxkeNCvw5xwGeIel-yk-38qyiT9KRFr6GHRrYIbCm7rTFm91d6BbN5d4ETWS9RudYWzKvXSEYvXw2RpRa5wkin7X5k_IL1IVxpvIOwbRXv4GUdqETRmQTCWLcRa6Fs_xA3Bysg71u1H8D1fll4i9Ejs5DS3Vwj8DL5tQZhuvecOoo2x6eJxU2XGL1_qpoKdfv22RJdef7UNQURKR92-ZQkTsmiGdfk3vaaKZL0Z3slW9xQABEPc5FtfU4pt-63jGjycS0qYRN6hYcjnzTeUCTI9rWsYDNZ9uAtFOoAcmv2fC7Q6TlYUzhLh2Ma8YISCuaW6YSBy-ZcKVGi_GA3Me9_VxLgzYSrG2GRxhBImMDr01WG13lHzExzPC4qoPLaDZ8mcE2SDzCsYUIESBhAdBkFFAWpdQFJ9kYHMowPqHuGBkzuWw=\"\n",
    "    },\n",
    "    {\n",
    "      \"id\": \"msg_68bf16b455d8819db1fabb18223e1fa60194eb11b43209d6\",\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": \"output_text\",\n",
    "          \"text\": \"```python\\n#!/usr/bin/env python3\\n\\\"\\\"\\\"\\nPrime utilities: check primality and generate prime numbers.\\n\\nThis module provides:\\n- is_prime(n): fast primality check for integers\\n- primes_up_to(n): Sieve of Eratosthenes to list primes up to n (inclusive)\\n- A simple CLI to check a number or list primes\\n\\nUsage:\\n  Check if a number is prime:\\n    python prime_utils.py check 17\\n\\n  List all primes up to a limit:\\n    python prime_utils.py up-to 100\\n\\n  List all primes in a range [start, end]:\\n    python prime_utils.py range 50 100\\n\\\"\\\"\\\"\\n\\nfrom __future__ import annotations\\n\\nimport argparse\\nimport math\\nfrom typing import Iterable, List\\n\\n\\ndef is_prime(n: int) -> bool:\\n    \\\"\\\"\\\"\\n    Return True if n is a prime number, else False.\\n\\n    This implementation is deterministic and efficient for 64-bit integers,\\n    using trial division up to sqrt(n) with 6k ± 1 optimization.\\n\\n    Args:\\n        n: Integer to test.\\n\\n    Returns:\\n        True if n is prime, False otherwise.\\n\\n    Examples:\\n        >>> is_prime(-7)\\n        False\\n        >>> is_prime(0)\\n        False\\n        >>> is_prime(1)\\n        False\\n        >>> is_prime(2)\\n        True\\n        >>> is_prime(3)\\n        True\\n        >>> is_prime(4)\\n        False\\n        >>> is_prime(29)\\n        True\\n        >>> is_prime(221)  # 13 * 17\\n        False\\n    \\\"\\\"\\\"\\n    if n <= 1:\\n        return False\\n    if n <= 3:\\n        return True  # 2 and 3\\n    if n % 2 == 0 or n % 3 == 0:\\n        return False\\n\\n    # Only test factors of the form 6k ± 1 up to sqrt(n)\\n    limit = math.isqrt(n)\\n    i = 5\\n    while i <= limit:\\n        if n % i == 0 or n % (i + 2) == 0:\\n            return False\\n        i += 6\\n    return True\\n\\n\\ndef primes_up_to(n: int) -> List[int]:\\n    \\\"\\\"\\\"\\n    Generate all prime numbers p where 2 <= p <= n using the Sieve of Eratosthenes.\\n\\n    Args:\\n        n: The inclusive upper bound.\\n\\n    Returns:\\n        A list of primes up to n.\\n\\n    Notes:\\n        - Time complexity: O(n log log n)\\n        - Space complexity: O(n)\\n        - For very large n (e.g., > 50 million), this may consume significant memory.\\n\\n    Examples:\\n        >>> primes_up_to(1)\\n        []\\n        >>> primes_up_to(10)\\n        [2, 3, 5, 7]\\n        >>> primes_up_to(2)\\n        [2]\\n        >>> primes_up_to(19)\\n        [2, 3, 5, 7, 11, 13, 17, 19]\\n    \\\"\\\"\\\"\\n    if n < 2:\\n        return []\\n\\n    sieve = bytearray(b\\\"\\\\x01\\\") * (n + 1)\\n    sieve[0:2] = b\\\"\\\\x00\\\\x00\\\"  # 0 and 1 are not prime\\n\\n    limit = math.isqrt(n)\\n    for p in range(2, limit + 1):\\n        if sieve[p]:\\n            step_start = p * p\\n            sieve[step_start : n + 1 : p] = b\\\"\\\\x00\\\" * (((n - step_start) // p) + 1)\\n\\n    return [i for i, is_p in enumerate(sieve) if is_p]\\n\\n\\ndef primes_in_range(start: int, end: int) -> List[int]:\\n    \\\"\\\"\\\"\\n    Generate all primes in the inclusive range [start, end].\\n\\n    This uses a simple approach for modest ranges by computing primes up to 'end'\\n    and filtering. For very large ranges with large 'end-start', consider a\\n    segmented sieve.\\n\\n    Args:\\n        start: Range start (inclusive).\\n        end: Range end (inclusive).\\n\\n    Returns:\\n        A list of primes within the range.\\n    \\\"\\\"\\\"\\n    if end < 2 or end < start:\\n        return []\\n    start = max(start, 2)\\n    return [p for p in primes_up_to(end) if p >= start]\\n\\n\\ndef _build_parser() -> argparse.ArgumentParser:\\n    parser = argparse.ArgumentParser(\\n        description=\\\"Prime number utilities: primality check and prime generation.\\\"\\n    )\\n    subparsers = parser.add_subparsers(dest=\\\"command\\\", required=True)\\n\\n    check_p = subparsers.add_parser(\\\"check\\\", help=\\\"Check if a number is prime.\\\")\\n    check_p.add_argument(\\\"n\\\", type=int, help=\\\"Number to test for primality.\\\")\\n\\n    upto_p = subparsers.add_parser(\\\"up-to\\\", help=\\\"List all primes up to N (inclusive).\\\")\\n    upto_p.add_argument(\\\"n\\\", type=int, help=\\\"Upper bound (inclusive).\\\")\\n\\n    range_p = subparsers.add_parser(\\\"range\\\", help=\\\"List primes in [start, end].\\\")\\n    range_p.add_argument(\\\"start\\\", type=int, help=\\\"Range start (inclusive).\\\")\\n    range_p.add_argument(\\\"end\\\", type=int, help=\\\"Range end (inclusive).\\\")\\n\\n    return parser\\n\\n\\ndef main(argv: Iterable[str] | None = None) -> int:\\n    \\\"\\\"\\\"\\n    Entry point for the command-line interface.\\n\\n    Args:\\n        argv: Optional iterable of arguments (defaults to sys.argv[1:]).\\n\\n    Returns:\\n        Process exit code (0 for success, non-zero for error).\\n    \\\"\\\"\\\"\\n    parser = _build_parser()\\n    args = parser.parse_args(argv)\\n\\n    if args.command == \\\"check\\\":\\n        n: int = args.n\\n        result = is_prime(n)\\n        print(f\\\"{n} is {'a prime' if result else 'not a prime'}.\\\")\\n        return 0\\n\\n    if args.command == \\\"up-to\\\":\\n        n: int = args.n\\n        if n < 2:\\n            print(\\\"[]\\\")\\n            return 0\\n        primes = primes_up_to(n)\\n        print(primes)\\n        return 0\\n\\n    if args.command == \\\"range\\\":\\n        start: int = args.start\\n        end: int = args.end\\n        if end < start:\\n            print(\\\"[]\\\")\\n            return 0\\n        primes = primes_in_range(start, end)\\n        print(primes)\\n        return 0\\n\\n    parser.error(\\\"Unknown command\\\")\\n    return 2\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    raise SystemExit(main())\\n```\"\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": \"input_text\",\n",
    "          \"text\": \"You are Python Developer. Write python code with document. Folllow best practices for generating any code.  write a program for prime number.\"\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"type\": \"reasoning\",\n",
    "      \"id\": \"rs_68bf16d33744819db24d785260693b880194eb11b43209d6\",\n",
    "      \"summary\": [\n",
    "        {\n",
    "          \"type\": \"summary_text\",\n",
    "          \"text\": \"**Considering prime checking algorithms**\\n\\nThe user previously got code for a prime-checking function but is asking for improvements. I think incorporating the segmented sieve method along with the Miller-Rabin test for larger integers makes sense. Using Python’s big integers, I could focus on deterministic bases for 64-bit numbers.\\n\\nI could implement the is_prime function using bases like [2, 3, 5, 7, 11, 13, 17], which ensures correctness for numbers under 2^64. It looks like I’ll need to consider more bases to cover the entire range accurately.\"\n",
    "        },\n",
    "        {\n",
    "          \"type\": \"summary_text\",\n",
    "          \"text\": \"**Clarifying prime testing sets**\\n\\nI remember that the deterministic set for numbers under 2^64 is {2, 3, 5, 7, 11, 13, 17}, which is sufficient for testing primality. Some sources say {2, 3, 5, 7, 11, 13} works too, but the broader set is better.\\n\\nFor a 64-bit check, other bases like {2, 325, 9375, 28178, 450775, 9780504, 1795265022} might be effective. I should include both primality checks and prime generation in the program to meet the user’s needs.\"\n",
    "        },\n",
    "        {\n",
    "          \"type\": \"summary_text\",\n",
    "          \"text\": \"**Outlining prime program structure**\\n\\nI’m planning to create a single module named `prime_program.py` and want to focus on providing clear docstrings and usage comments without heavy formatting. The functions I’ll include are:\\n\\n- `is_prime(n: int) -> bool` for the trial method.\\n- `primes_up_to(limit: int) -> list[int]` using a sieve.\\n- `primes_in_range(start, end) -> list[int]` that employs a segmented sieve for robustness.\\n- `next_prime(n) -> int`.\\n\\nI also plan to implement a CLI using argparse for subcommands like check, up-to, range, and next. I'll keep the code tidy, aim for fewer than 200 lines, and include unit tests in the docstrings.\"\n",
    "        },\n",
    "        {\n",
    "          \"type\": \"summary_text\",\n",
    "          \"text\": \"**Implementing the segmented sieve algorithm**\\n\\nI’m aiming to implement the segmented sieve algorithm with a clear structure. First, I need to check if the end is less than 2 and return an empty list if that's the case. Then I’ll calculate base primes up to the square root of the end using a simple sieve.\\n\\nNext, I’ll create a boolean array for the segment. For each prime, I’ll find the first multiple within the range and mark off the non-prime indices. This approach helps avoid memory issues. I might also add doctests and keep supplementary usage details to ensure clarity in the code. It’s crucial to include docstrings and inline comments while avoiding heavy formatting, except in code blocks. Now it's time to finalize the code!\"\n",
    "        }\n",
    "      ],\n",
    "      \"encrypted_content\": \"gAAAAABovxb3e3mvRauRpd7hdTpkS5Dh_bJB7ZjSbx6MdJmq7aTnrQRYQ9j_IEvOeWgWn4ycbyKQbH94-tToGOux-URjFEPJtVpyuZCuXGx3XIuhn3V9DApaPhHKhkTefHuJc_mzMHHoV6tiQbYr_yc5Z2ssmj1T_VaGylwP5iyIvL4LVQRRKzK7QUnfHF3z_b8sGk2Sz2ypC39XCBfPQVGZ1wZSchYIpma45FMzy_FgDAN_cTQQlCof3iDOI8e6VLiAZlzwyGNG-5yF-48c9MEvdW5pX4551gd6KTRPhfAaRWVnpSiTY1ypb0aScwjMrUSns9VOj2yLkzMNdxl-zsWL1vykCIcxtlNyXHahghmMxtEsBiLeHNVRlzui25ClmGMrjnZa_MaNRdyLjg8G91UET8_fEiNMJM00oPnG8A7rC-1UW0w7QJCaf4S7BP3z73xT6lE6yExYCvrOFyyWatKTVycMvdCAaJupjmCrrDKuMStz4JnyLMNwWtTNljC7nivjlfjOezn5cHRUpM9O5z-jUZRtSVkwRrUAYdjBl9_xHa1dWeQfubHTUEkGS6vdvrT3vyJzfnOzRMe00CeEWTw5QkyhuFpz3-Cj1V4GRmhVP5VCunjj2x0YOgiY8L6dViEJqTxNb_5a2k-jjZnVrWGWozDAjHhGtek8vYFMR6RS2U8KgyOy7GNPk1TE9uUzx_-1R5OjdGk43vz5mEs_QuSpn-9gxCgjWgtANjiIHazXwU1JzCLfGITYSflOu9a290Fcd18MUpHSjhiBZ7f1NCYECDNrAuB4Kqezpc_piIaDYvKNNtOWTJkhFYNGNintbMeZmsHl49GkhuOPrlkMhWX0gDIXmWfIdnIEPNcaNApJLJWQvglwa9gQB5f3RESMg269dL07-Dee01djimaXa3o3foAB4fdhw2NOxxjYl9LdnYC3shcKr-ZSSC6pHrx760M9BtzT8rYwT6hbSkfouadTzH-iS78Zp4sFKBOgL1QT3-_59KWThPWoIMHTg_z8dVaaaJTgBHnTSBVVtKE6rEpnI4RC49t0J8v1_VagryloAKqU17yzgXXc-kDGmM8g60E46VxLpqNPGkzmiO1AHaCWu_9bPHLTedvN8OVFDlxZzK6WFfDJ9h41L_WiX7Hwapt0rlF6VTqbIif5qj7uK4XztS4U6pt7kYo5EMvDROQnTIRsDO3t3XQ6UKyRIzVGSzF2WTxUZHNwNSLxg1g5RmCAHPcH8WzYYOWP4ImAdO-62k9k8153x_8ZlNeUVWGSjaH6yfXuZT4SZjgnECBs2bEk9RQH6HPGeQuJLpPguRfdiO2ToUrTRpLuHFQOukPFyd8CsxG1cH4SUFbpSUNB8KkeMJxJTq3nK230E9LOb9pI_eViwCo8tUPty64HeyvYdB7jqyhT9WEqAxHkRg5mRF5RWCwadgexDpQhkKuQ2oCXvXiarJklb1AIqHa39tZ2OTUhn17zTyfKN4_lNQUVOj5iCn5XH9KpwRndGfuQMFWbpXHqRIpi9K7TSYkaoOZomgflvvE4kIi8TLSSr-OrLSS2etxbiqgL7N0wDF6WbV_58MBDnlNsOO-6nlUc0YLUBxsPNXXgR-CI6rUDJI_a_CcW6Ay526oxF2RgUKTfLfXKxilqaduJAkp6p4UAUV52-00SBDbdQ5noI1X-CqwOUvHkpoHG-rBFQmmhC_NqNMOeNIcE8-cqo5IVFU2UCvMObBWktW-EqqBRxjegzcdBAcDlOgGAkdkUhxvJxbcS7wmuxppsCV7Kv4aUnciAPQGtz8rziHbMF3oVgF8FykmNY526zex6uIue1h1HvJLGt-zLUlkb3kBtT-JCCmrIDUQ-wug1ju_psy70srRB_guCDfT9amsKbI0P6m9mHjqIs_gJrE0DiPwG1Xd0AP08S8zeNw_7mqhTJdADyxhCEp1_aAyBPtG9EBCjvV1VaghRUGHuSRjwQ9hR2xGGctlgVorMB8xqlEDqRwXQT-9J73v9DMDDOYCeERLkRJpz0vN5bEceaMyiin0WPat2nJH63-pTEzWTjMqGky-6cFIoLTttTIq5Xx_cu1eJyoQ_vjLGDy6PA-WR32dKAxFOIQLMuV0TKcqZ2_vL8TpwQcew7ik77qozmJk4Ochqi_G8LEJN52dvZR_HrEHxHE1vEjoQLNcZSxS1EjE3N5fwebYFfNlt2GJ-Fh4P2wHKKKw7YzjWsctCJk9YLzCLv-wQ7vNtQc__ZLNfsebfm5P_HnQnwhFxZftO-Gxy7O8UpC8NH5wXL262dlCash7NDBujPuLEbzr3xVs-Qb6IfVK7NnZhYwXAbK3J5MJ37VBs03JLoks03T8tDMvPxq4wJI4in2CyNpYZgaPiiHhiUp9tKONeNklvqKrywEImCOaC8XCrXsIfRTc3-5WN_gOMVDoRf9yi9-Su3QZywWMGUjDLjkiFk8AOtBJ5sDvY_-aseqlwPu_Be_qOnVNrr9wQ--k1fLOoXx5mk7WNz12db9t91Jv2MjzHc5AzVPMQjWPE0MRUZH6euyX9irdS_EgUjfEVv21S_NrCC6IthltlXr_bHdaga8suOt675FFvILtJ0Y9l7d3NogJ_FJjlH1TSMW9SALK-82hWY61NgpPg0A37_HCCrnOcvRK2ZY4_4QPcFYYWeiWq3IXSQllWVodwg9FbUC0f7oqGvwK154BDkZTgTSC-d0ar5bBn8kNfbuGxOVoQ6KZTPV_1EgSxCpID8SRYQTlVKmfZ7lxd1VloBlF4NHsA_qHcDAxj6Q-Kw50EBOHyoAqnoEZevRRDh3HD_G6FPNr49IS2jHCqcRjbcPe-sWhY7l9UYHGTMgGpx-xBf3kdNhwBlyWp7cEteQ2K9p0uoMxyoPstRB_B6iEOzYGRHhT90et-9LM9SPBRh45iybnFAFz-1KnaqFxQslmac5V0rMyx5-jczjXSXvGmuHpjB3QHZ9MP7YpW7fbpbYgYe0ynoW6l7TVwDTlqNFKuF1wP3L-cbx8qZtXDaM8ImLSWOKLsd3MmJqFFJOKiuj8tHpQo2poDtAxbD585o7OSoi5EBlkIDPu_sDgvQD3F9XY89Kjb-3urlV3F2I3B_9eM46NEnvribCR2JKB0TKPZo_MzcAA8EgzOBs6-6o5UASS-b5gIL30q8qL_OBCf8_S_cuQnpgUMYQt4HQkDSjnsVymkrucc8FFnvrj5yOm2NGE5R3-n2TH_TVjxU_AqcoErWBtLAU5aEjjzTlTswpDcRJCwdPNxhntrMazq1Ava5Ww7kQQ9lUuf6epliAhUn94X4Bz9dyIazxZBIvkmqY4fcCeDqUR_AXqeKw760QAVSv_RdR0I8fqbF-Kcfr12rA0NAO9xSCsNb35a8_45xG4-q5lLVHqR7Y2eNFjHoKATr3aigazub81iaCu5W1kNYz0MTDQXSyWZc_Cw1SlHz5pHhkH_reUa0GBeK8X-sIphVjYj5E-MvLG-f9YntPAJdg0wL2WSwFdmJq00LidUp63KnbvhQnRnqogTBD7HDhb09WWTllAH-6Dsdh6llMFWmjzRWfi09JMwDtyOVDwClEEkLHP4WtPnH6B3Z5bG89j7uSMskqeAZuCe3YxTQy8HNnJD2FuAXOVR4LgoaQv_L_IIrehX1WmV4rmDlVtUWFZ2ZAquvdoUTQ7ez25fgh9d3gx4gklJaqz0YpiDKi2YynYT9ThxcEwf7cuy_MszBY7PufO7iz0MQUYsaF9mCkaZSWKuQRk6I6fvpRSbsg5Bu2V6cObxB_qayoRVuWjxeTG-GZVGLxsS4rI-OZFs1CxqpliSpqV7efAkmOegxEgyQK_FcCxgCAHY7zF9llhu6aRqol-cp7U4wr0LHx8l-aCWJ8qgOuZs9ayXV3-5QSfv1K7ycpRpqWfkYiqmX7Qf9C64X_So9i5i8M9g1AcTcKIkr_9thJ_RsYS1be0NbLpgqyC1pKWir-bJakAiDz15OUG3PnAehDhxMPFqGJQrAsFf1vUNB0sHlOQhkn7WdKnPGN479NwMAIgRBq2ffNl9uLrkozKOOWm-aZVsn-W6VK6uIGecxnMMdvM4r9Jdr6uXJMnwg8kD3ljszLkExxfhlaJ6xa_gPiiGRx4e80VikxXwq-ZwkqaZyBrwBgCVcpr8Nd4nyifRJr6Y0BFNzJ-dsD4NmgDCAWc_8C3UlWigoY7LbcGPLzGH8Kj7jB85Lj6XRItvlPChUJ9zlcWM4jn4i7ezGDiYEqMQWKje4UE3DPtdfXEJ-BRgZAfRXhf7umlrPaKJ4PeZngNa5MS7ctTO--074Fjx6rKuhmpOmMl1-wV1ko96ag19EsMLM42rfhJwwur9xql-XFbgIQLgBkMvmZ0JU-CNfXyL5c8TYIuN_lt-s9HBIJiD3cRdtqORe4CMsG7WuRt4uAb_zOx1xuVRpT74HRuFzZ6FWtHPmhMzI6AfifmGLxmmdKdgMJDnHBAA3JekO8GD8I6XEjH34AWonIONkNxVfoj56cB-G9q74AWN4mIUt2BU166Y9zWK0V-1z5LZI8NWzXVI9vF7YA5HVv76UAkJuPwUBnC1QFz5tApjSBNoVNYLCUyEnhlt8kjUX19oPJT_DjV8Hr3ZF3RVrZqod9EQIPaa9I1xbB7SrjspzjoZ44M2pgoXjvH4o5ymae_3AfEXkwjGd7RXGONSXO3SGdlkGe6iHrkKNW74DZnX89RE_zTmHOqprNKMm_Sbw8XHsgrZoq_WW-x2_AmjnmSy7C4C48wWBRS-8b5pGjyzf_vHdQos7S_RySIJu9-lNvkneyHVwMlZJY2UBfd6_J9_Uea2cpFsMNutyw01bNkhTEhRXLepd_0v6QBv5amWlfeom7-25OqNJCz4qbkqqjsx1HedWYAkBbAJLIiRYiXp1LWlvmMnXSUiDc3EgrTcHBjL6yplpzTXusDp8fXRgw8gwNu2TJ26qioQKx_YWymy00xSuQExUdlJULxgRAwbA1NBuMeCVP4cmA8WjePxDxu_hlzRUEf3ZTv69zOa2wtOMor5gz0oDQ8QaAYZuzVkgOrOScQ4xFIx_zsgZlE9eXjfrX6b9i3PRCkxj6A5DPPWld1kWg2zS4OCMg43RTqzAApgGohfw766YJ6SYzz39Ght-m6_76d16XTwp8rR4uqS3B2BV4nJXpb9jBRDbbLU5ApWU18VsLXW0G1Khg-DsF8uUpJVqGf3YkMRs1RhP2MzBm3gw7VAm_L3uEHt_1fMlKk3tK5ACcH43UFcF43KxbYVXpPoB-v53s6V__-_apstgVhsgiCFl9_wThDtioGvfoLz5HtSk00Hldp6bGNjV7Fa73elSrqFu1cSuhdI0BVWCU_ufrRFbS2orT2PlGrd-SeUPvmB3vKTWfVJM4YlXPH0zMtNmbK7JQ_LquK96VJT7wU36oJTVm9BP4IzZpHWEat9jy4O09lU3Hv_7UU-drBoEy3uv2jMHRw3cdb5tAcxy5PPU5iLaeH2y6JJUxYEUsCeoldr0PWd8UwdA_94M9RT72Fm2r7xbDJa-hOeMHA1U8x_Cx9xstimtqy3zLQCbVF6FzdvN-xYqzo5tTsLn-pk6-CDrsYxScto7oHwucFI2PxgpVnBMHfDIdEgJzYhvl8NKmwSx-ycReaJMaOcBMVg2qS3yzsRgydCGXE_Fsr9POrdDoKa6KAVrYaXyEcl7_onSsaQ5pHFHaMX5ElOvqnM5XV12m6FtbrLu3IGyhMVHlZHelz9zOa7QPqTyP7SYZaE7skqfTw913Uu-EBHSHEFzxgSoO-LyTTGNpekKA-8gZsuJyP-mKn5ocILht1I6p9TC-gdT6x_piHgqQgcUQe49C731802TotIgQLTwpFQ1rJLqImPTbkqqBF_UoREp-PCqX0oioi_Ao0WX9WILYjJ0hz1e73398dfYfi5bFzf6AqyYeEVYDFyfVqATht-YbB5Naqcm90KFzFgT8rYQ3rEdDEGOfNzPKbqj-QJLKB9mWRZZ6PBL8ngZNStXSFbyQbwdGabmH4Lz_qX8Vbz9hCYMCCFAo8MEsm_zLAnklrB4Xjl2gU8a5845myM67oOlR1PABfoMfXPK0tUX9zH61FeupfvUd6I7cwVB594zGfboENH0d2niYYN-u03dh0QiPEggjXeNCsB2cOBHUyO77tMyovltvz33Ije7hUSVk0DttSpxQKyfJBqwVrsB9GB40H16Hdz_wP8qmOmN5mWmm7UqvlBtbJRgD4Sm_XCnCn_9Jq7H7hEzbvERk79xv_5bNcy68dwjw1nEeqMvAJDSm2jMoLbOz9rxGUbKaBaUu2dkFwwg9Ze4v_VNmBcvDvwqwJbh62uUwqUJg-8jw2UC9vPa7p1D-aF3eFYTuSCEx4n0jXEEVat2M-MJ_sfcXevPSyrhJIndJwgCP1wh0MV0RfIdWx2esEdy7YI8jvENUE46uRISpfKjPVGcKS19gl6BR17QrnZUNDpqZe-HZ9_rVeVohTp6EMkffFv0i_kixpoExJPYQUbuke8gZ22gXXYBbt5TQpgbOiL5ENMy9pwByIKtmGeOJ7XUg4GQdAF0yjvl6khdLR05s951i-j6suVP2QPWXxsmDNVbWvxSCk_mZ2OUtrTmFMh2v-Gdpp8rtKb80xBIP38mJEi3qZp36P9tB6XvHeY6TY3E960R6S4kRxzbnJoC9UL7-r0YlpjozNbkN5sVBiEpWKrZ4TiVAqcBIUevtA=\"\n",
    "    },\n",
    "    {\n",
    "      \"id\": \"msg_68bf16e6bce0819dafe8056972e74e230194eb11b43209d6\",\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": \"output_text\",\n",
    "          \"text\": \"```python\\n#!/usr/bin/env python3\\n\\\"\\\"\\\"\\nPrime number utilities: primality testing and prime generation.\\n\\nThis script provides:\\n- is_prime(n): Check if an integer is prime (efficient for typical use).\\n- primes_up_to(n): Generate primes up to n (inclusive) using a sieve.\\n- primes_in_range(start, end): Generate primes in [start, end] via a segmented sieve.\\n- next_prime(n): Return the smallest prime strictly greater than n.\\n- A simple CLI for interactive use.\\n\\nDesign notes:\\n- For primality testing, a fast trial-division method (6k ± 1) is used,\\n  which is simple and efficient for 64-bit sized integers in everyday use.\\n- For generating primes in a large interval, a segmented sieve avoids large memory usage.\\n\\nUsage examples:\\n  Check primality:\\n    python prime_program.py check 97\\n\\n  List primes up to a limit:\\n    python prime_program.py up-to 100\\n\\n  List primes in a range:\\n    python prime_program.py range 1000 1100\\n\\n  Get the next prime after a number:\\n    python prime_program.py next 100\\n\\nThe module can also be imported and used programmatically.\\n\\\"\\\"\\\"\\n\\nfrom __future__ import annotations\\n\\nimport argparse\\nimport math\\nfrom typing import Iterable, List\\n\\n\\ndef is_prime(n: int) -> bool:\\n    \\\"\\\"\\\"\\n    Return True if n is a prime number, else False.\\n\\n    Method:\\n        - Handle small cases explicitly.\\n        - Eliminate even numbers and multiples of 3.\\n        - Trial divide by candidates of the form 6k ± 1 up to floor(sqrt(n)).\\n\\n    Args:\\n        n: Integer to test.\\n\\n    Returns:\\n        bool: True if n is prime, False otherwise.\\n\\n    Examples:\\n        >>> is_prime(1)\\n        False\\n        >>> is_prime(2)\\n        True\\n        >>> is_prime(29)\\n        True\\n        >>> is_prime(221)  # 13 * 17\\n        False\\n    \\\"\\\"\\\"\\n    if n <= 1:\\n        return False\\n    if n <= 3:\\n        return True  # 2 and 3\\n    if n % 2 == 0 or n % 3 == 0:\\n        return False\\n\\n    limit = math.isqrt(n)\\n    i = 5\\n    while i <= limit:\\n        if n % i == 0 or n % (i + 2) == 0:\\n            return False\\n        i += 6\\n    return True\\n\\n\\ndef primes_up_to(n: int) -> List[int]:\\n    \\\"\\\"\\\"\\n    Generate all primes p where 2 <= p <= n using the Sieve of Eratosthenes.\\n\\n    Args:\\n        n: Inclusive upper bound.\\n\\n    Returns:\\n        List[int]: Primes up to n.\\n\\n    Notes:\\n        - Time complexity: O(n log log n)\\n        - Space complexity: O(n)\\n        - For very large n, consider using primes_in_range with a segmented sieve.\\n\\n    Examples:\\n        >>> primes_up_to(1)\\n        []\\n        >>> primes_up_to(10)\\n        [2, 3, 5, 7]\\n    \\\"\\\"\\\"\\n    if n < 2:\\n        return []\\n\\n    sieve = bytearray(b\\\"\\\\x01\\\") * (n + 1)\\n    sieve[0:2] = b\\\"\\\\x00\\\\x00\\\"  # mark 0 and 1 as non-prime\\n\\n    limit = math.isqrt(n)\\n    for p in range(2, limit + 1):\\n        if sieve[p]:\\n            start = p * p\\n            sieve[start : n + 1 : p] = b\\\"\\\\x00\\\" * (((n - start) // p) + 1)\\n\\n    return [i for i in range(2, n + 1) if sieve[i]]\\n\\n\\ndef _base_primes_upto_sqrt(end: int) -> List[int]:\\n    \\\"\\\"\\\"\\n    Helper: generate base primes up to floor(sqrt(end)) for segmented sieve.\\n\\n    Args:\\n        end: Inclusive segment end.\\n\\n    Returns:\\n        List[int]: All primes <= floor(sqrt(end)).\\n    \\\"\\\"\\\"\\n    if end < 2:\\n        return []\\n    return primes_up_to(math.isqrt(end))\\n\\n\\ndef primes_in_range(start: int, end: int) -> List[int]:\\n    \\\"\\\"\\\"\\n    Generate all prime numbers in the inclusive range [start, end].\\n\\n    Uses a segmented sieve to be memory-efficient for large ranges.\\n\\n    Args:\\n        start: Range start (inclusive).\\n        end: Range end (inclusive).\\n\\n    Returns:\\n        List[int]: Primes within [start, end].\\n\\n    Examples:\\n        >>> primes_in_range(10, 20)\\n        [11, 13, 17, 19]\\n    \\\"\\\"\\\"\\n    if end < 2 or end < start:\\n        return []\\n\\n    start = max(start, 2)\\n    segment_size = end - start + 1\\n    segment = bytearray(b\\\"\\\\x01\\\") * segment_size\\n\\n    for p in _base_primes_upto_sqrt(end):\\n        # Find the first multiple of p within the segment\\n        m = max(p * p, ((start + p - 1) // p) * p)\\n        # Mark all multiples of p in the segment as non-prime\\n        for x in range(m, end + 1, p):\\n            segment[x - start] = 0\\n\\n    return [start + i for i, is_p in enumerate(segment) if is_p]\\n\\n\\ndef next_prime(n: int) -> int:\\n    \\\"\\\"\\\"\\n    Return the smallest prime strictly greater than n.\\n\\n    Args:\\n        n: Integer from which to search the next prime.\\n\\n    Returns:\\n        int: The next prime after n.\\n\\n    Examples:\\n        >>> next_prime(3)\\n        5\\n        >>> next_prime(100)\\n        101\\n    \\\"\\\"\\\"\\n    # Start with the next integer; ensure it's odd (except for 2 case).\\n    candidate = n + 1\\n    if candidate <= 2:\\n        return 2\\n    if candidate % 2 == 0:\\n        candidate += 1\\n\\n    while not is_prime(candidate):\\n        candidate += 2  # skip even numbers\\n    return candidate\\n\\n\\ndef _build_parser() -> argparse.ArgumentParser:\\n    parser = argparse.ArgumentParser(\\n        description=\\\"Prime number utilities: primality testing and prime generation.\\\"\\n    )\\n    subparsers = parser.add_subparsers(dest=\\\"command\\\", required=True)\\n\\n    p_check = subparsers.add_parser(\\\"check\\\", help=\\\"Check if a number is prime.\\\")\\n    p_check.add_argument(\\\"n\\\", type=int, help=\\\"Number to test.\\\")\\n\\n    p_upto = subparsers.add_parser(\\\"up-to\\\", help=\\\"List all primes up to N (inclusive).\\\")\\n    p_upto.add_argument(\\\"n\\\", type=int, help=\\\"Upper bound (inclusive).\\\")\\n\\n    p_range = subparsers.add_parser(\\n        \\\"range\\\", help=\\\"List primes in the inclusive range [start, end].\\\"\\n    )\\n    p_range.add_argument(\\\"start\\\", type=int, help=\\\"Range start (inclusive).\\\")\\n    p_range.add_argument(\\\"end\\\", type=int, help=\\\"Range end (inclusive).\\\")\\n\\n    p_next = subparsers.add_parser(\\\"next\\\", help=\\\"Find the next prime after N.\\\")\\n    p_next.add_argument(\\\"n\\\", type=int, help=\\\"Starting number.\\\")\\n\\n    return parser\\n\\n\\ndef main(argv: Iterable[str] | None = None) -> int:\\n    \\\"\\\"\\\"\\n    Command-line interface entry point.\\n\\n    Args:\\n        argv: Optional iterable of arguments (defaults to sys.argv[1:]).\\n\\n    Returns:\\n        int: Exit status code (0 on success).\\n    \\\"\\\"\\\"\\n    parser = _build_parser()\\n    args = parser.parse_args(argv)\\n\\n    if args.command == \\\"check\\\":\\n        n: int = args.n\\n        print(f\\\"{n} is {'a prime' if is_prime(n) else 'not a prime'}.\\\")\\n        return 0\\n\\n    if args.command == \\\"up-to\\\":\\n        n: int = args.n\\n        primes = primes_up_to(n)\\n        print(primes)\\n        return 0\\n\\n    if args.command == \\\"range\\\":\\n        start: int = args.start\\n        end: int = args.end\\n        primes = primes_in_range(start, end)\\n        print(primes)\\n        return 0\\n\\n    if args.command == \\\"next\\\":\\n        n: int = args.n\\n        print(next_prime(n))\\n        return 0\\n\\n    parser.error(\\\"Unknown command\\\")\\n    return 2\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    raise SystemExit(main())\\n```\"\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": \"input_text\",\n",
    "          \"text\": \"write a program for prime number.\"\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"type\": \"reasoning\",\n",
    "      \"id\": \"rs_68bf1746f198819da1fa42bdeb7762230194eb11b43209d6\",\n",
    "      \"summary\": [\n",
    "        {\n",
    "          \"type\": \"summary_text\",\n",
    "          \"text\": \"**Creating prime number program**\\n\\nThe user has asked for a program for prime numbers again, even after I provided the code twice. I need to keep it concise, so I’ll create a simple Python script with a function to check for primality and another to generate primes up to a given number. I’ll use docstrings and type hints for best practices, ensuring it's straightforward. It sounds like I should include argparse to handle input in a clean manner and only output the code without extra explanations.\"\n",
    "        },\n",
    "        {\n",
    "          \"type\": \"summary_text\",\n",
    "          \"text\": \"**Simplifying the prime checker**\\n\\nIt seems like the user really just wants a minimal program with a straightforward input prompt. I’ll create an `is_prime(n)` function and a main block that reads an integer from the input and prints whether it’s prime. Adding a feature for 'up to N' might be overkill, so I’ll focus on keeping it single-purpose. I'll make sure to include a docstring and the `if __name__` block, which is always good practice!\"\n",
    "        }\n",
    "      ],\n",
    "      \"encrypted_content\": \"gAAAAABovxdU2wbmV8-LkljDvZzpeMqXzxDNCUmdGRXVP9hnO8VuMX7WysbJLxkEgsB70OLtgTWOyB9Paap1C2qOimMqQSKCaW9DKsz4t_pwraHxwXBJiqYK1CIJevwm23bQu0sxrc4Kk0kom3WymOEQonle737AUwerphfEKwXuCZ-3QpCCLZ1Z0HIXljmAhJuFDKNZWil8rruU_tDKxzD7BaCcLpX_XEE2PyRu1UUqsnb70zAoIe96ZwmjHCIS0FzY7S7SURLwM-MWmd9Q6bSPNt3Khe_clhgMnbmN-ooMEI6W54TUEiMQjNHfA2JHhLF6BgoDiC50khtpsGyYNMFJnZF7k-n5TW5sHvAWJEVmHCT6_y5LTuiu_Uqk0t9e0U41jXGnwxJO9mQDoUluBx8fmcVPAizmK1d4B2lG5CX-lORGAwzeO1W9n-XcF4Z_q7PcUA2fmTgPegAi4dad4KMky32NbAptVBSlV0haDzsO90ygFU8OIkXylb5FvM_n6cVRexaWaZmM1jbv5_ehzEDp0KpoSiNRIUBozjXvjcfz1tbexXx6n4xuLgCvtZAI2BxgL_o0cW6KDlUrW-jceGxOd13IC-lj9mXWF8XOmQdS5jnHReJnEOizrHuHWEdDkV7RljudGimXqJZmSx1iRiR0hoQtGMYw70ydtXY_4dPQBXv3niMgN5EgqTF2J1ccnqN9uy4s19zteoJB8PJCtcSOtn_9xOhr-crS8QFZJcLf5VWSOLLRVVaLx-KfsmsRWjRN_TvlG3fYHBh4g19hfs8Ym0ya8RG5hrrhRY0YzHgX5jHBsFh2SbkT263GUrqUZfeg2d1Fr_B107_xzRC7BSErxfG9trmsUt45pjfJ_tIT75HNVpt59IsNLN8iYAtq-FXD9AizeY48R2FRcheKjGYLuXp07TF_cN6CDelNVVXaBOoV_TVaTLl2gwf8R3Z7Rd_BxheO-0-VGKkdg_4oweyb0eflxMorm8Aoeds7qYM259GTVajwG-E907i71RIoxXP2izsXnDAEuTyFSrgX2RzdiirSpIAEIsIKuYW8P7iRamxrtDiME_fmJxSJfx7ebJXc9BcGz-YEFjQHPkLGYkR0x0Fpl-7ByNOmey_VQ6WqKYxxfAu1K92fJgQjoaF-eDn3wXD_pavjRBA-vS4gd6MdH2OG0-m2xzmjbtfeIOZEizHeZIiJvPMZQCzKiHWwUte3hYzd93ZNuMUp9H-NKAtbgZktmt_ZOl3sm_kTtH5Avk7tF3LtvjG8Tq-gy6v5mKNJJVvbdppZRif-b-YzFlc5ABNABGkOYyQEM1UNXz4AaEmTHFmJh6juawjBP8xEP9gxbaDLbTSGFhIALWTaeCaJP9pCiC73W_O2QWi7E81F0LQFtdYJFVL6mqkJEhDciRpYbeZ-feT1Xylv53Rw810zvlafrSe4hIOB5SXt1p16ahzWvZ2GZPJyqGl9MCOJodBnYBbLxkaNxxJUGopHg_h2m7gU5QrhGNcketwQ_Wd2kbLPlI6X3esVg10b1kpoAo2Qss_kMUOGen859IXYPnmGZvvMnbEXsyAkwIrEtTYYcqVctQM7UsxJWMfjeEBsF3VVMsSI04NWk7ZsrCM_OSrOj7IYcdOY7QtPiX1TlNiMa8_rSrbKO_LW_SxwOVAVzyy6-cZcFBJzMYQrrhyMoDxphtUPGyC0HG15TVMnn8EInw-QmQUy9OKGhWrsgq1m8ZKtaV2Kf6hCHCVzDkn5iM19FZkn5IaAM1vApdl31xuj40SvboOX0wgO6pRHuYCyWaCpC9tN4BY2g5SXQa4D5CBnl52TtQgXCMdi9eZyoYSlnhceFfkfalhZNQBom3UfSNVKOswiL8OlCJNrbeDc9kX4pDP0fbXYUzzCHfp8geD4Ios4BBDbXbPiCTla2VVNl7ToPxPM2FqpNQNH_L8P0F0f2RFWKZGyNfakA3xS6FVBE8fI4rCRr8_-KEiJ5-KKOW9RO1lmHb4qi5RiL2NROk4rls8jFL2i6qkbRQsD3x76pjPqNiiEeOUVV6KfHZMAMuRyymg3-OwGiEx-n1D1Y3yg1Yz_ygE-nWlL3iP5lo3xWRS2raHNWrX6zP79l2ggsvKKuxq0nI0YZ6wHZ_zZmG3bPUYd-eKyWBHp5mXpSldTUSXBo8gnWCdqlH6zNlX7UO6J56vpimNTO6nmTTpiMU5cjvzVObOiFq5LWv6OkBKWqduO1KaMOAMz69oubnea8BnoNmBIknFC2TXQiFujjARbexN7i1wKa0EnkN29WN9IemRHkNsHViRCDOsyjqVTZ6cgyrpjST-1ygULrxgnGqUAhsxDKTaOi95Ad2xyCAtSoZYtuhGVCmM54EYK2PxOz1S7CalKVdid-c-u4hA2Dr1EXpe9n8-mQUxIDW3AC8dsYe87oEdV_nGqw4shlmsI5q92sLUJvNqM00QKRuHobKLC2x6-OZgAamUCkeqs5StgfW1CSiMeV4lbUqCOS-7CtU9Uqm5cyHFB\"\n",
    "    },\n",
    "    {\n",
    "      \"id\": \"msg_68bf174dd330819dad932c6b04f2c76b0194eb11b43209d6\",\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": \"output_text\",\n",
    "          \"text\": \"```python\\n#!/usr/bin/env python3\\n\\\"\\\"\\\"\\nPrime number program: check if a number is prime.\\n\\nUsage (interactive):\\n  Run the script and enter an integer when prompted.\\n\\nUsage (CLI):\\n  python prime_check.py 97\\n\\\"\\\"\\\"\\n\\nfrom __future__ import annotations\\n\\nimport sys\\nimport math\\nfrom typing import Optional\\n\\n\\ndef is_prime(n: int) -> bool:\\n    \\\"\\\"\\\"\\n    Return True if n is prime, else False.\\n\\n    Efficient for typical 64-bit integers using trial division up to sqrt(n)\\n    with 6k ± 1 optimization.\\n\\n    Args:\\n        n: Integer to test.\\n\\n    Returns:\\n        bool: True if prime, False otherwise.\\n    \\\"\\\"\\\"\\n    if n <= 1:\\n        return False\\n    if n <= 3:\\n        return True  # 2, 3\\n    if n % 2 == 0 or n % 3 == 0:\\n        return False\\n\\n    limit = math.isqrt(n)\\n    i = 5\\n    while i <= limit:\\n        if n % i == 0 or n % (i + 2) == 0:\\n            return False\\n        i += 6\\n    return True\\n\\n\\ndef parse_arg() -> Optional[int]:\\n    \\\"\\\"\\\"\\n    Parse a single integer from command-line args if provided.\\n\\n    Returns:\\n        The parsed integer or None if not provided or invalid.\\n    \\\"\\\"\\\"\\n    if len(sys.argv) >= 2:\\n        try:\\n            return int(sys.argv[1])\\n        except ValueError:\\n            return None\\n    return None\\n\\n\\ndef main() -> int:\\n    \\\"\\\"\\\"\\n    Entry point: reads an integer (from CLI arg or prompt) and prints primality.\\n    \\\"\\\"\\\"\\n    n = parse_arg()\\n    if n is None:\\n        try:\\n            n = int(input(\\\"Enter an integer: \\\").strip())\\n        except (ValueError, EOFError):\\n            print(\\\"Invalid input. Please provide an integer.\\\", file=sys.stderr)\\n            return 1\\n\\n    print(f\\\"{n} is {'a prime' if is_prime(n) else 'not a prime'}.\\\")\\n    return 0\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    raise SystemExit(main())\\n```\"\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ],\n",
    "  text={\n",
    "    \"format\": {\n",
    "      \"type\": \"text\"\n",
    "    },\n",
    "    \"verbosity\": \"low\"\n",
    "  },\n",
    "  reasoning={\n",
    "    \"effort\": \"low\",\n",
    "    \"summary\": \"auto\"\n",
    "  },\n",
    "  tools=[],\n",
    "  store=True,\n",
    "  include=[\n",
    "    \"reasoning.encrypted_content\",\n",
    "    \"web_search_call.action.sources\"\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c8747a",
   "metadata": {},
   "source": [
    "1) Instruction Q&A (system / user roles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545b84a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector databases are specialized databases designed to handle and manage data that is represented as vectors. In the context of enterprises, these databases are particularly useful for dealing with complex data types such as images, audio, text, and other unstructured data that traditional databases struggle to process efficiently.\n",
      "\n",
      "Here's a simple breakdown of how vector databases work and their benefits for enterprises:\n",
      "\n",
      "1. **Data Representation**: In a vector database, data is represented as vectors, which are essentially arrays of numbers. These vectors capture the essential features of the data. For example, a vector might represent the characteristics of an image or the semantic meaning of a piece of text.\n",
      "\n",
      "2. **Similarity Search**: One of the primary functions of vector databases is to perform similarity searches. This means they can quickly find and retrieve data that is similar to a given query vector. This is particularly useful for applications like recommendation systems, image recognition, and natural language processing.\n",
      "\n",
      "3. **Scalability**: Vector databases are designed to handle large volumes of data efficiently. They use advanced indexing techniques to ensure that searches are fast, even as the amount of data grows.\n",
      "\n",
      "4. **Integration with AI and Machine Learning**: Vector databases are often used in conjunction with AI and machine learning models. These models generate the vectors that are stored in the database, enabling sophisticated data analysis and insights.\n",
      "\n",
      "5. **Real-time Processing**: Many vector databases support real-time data processing, which is crucial for applications that require immediate responses, such as fraud detection or\n"
     ]
    }
   ],
   "source": [
    "#GPT-4\n",
    "resp = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0.2,\n",
    "    max_tokens=300,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a precise technical explainer.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Explain vector databases for enterprise in simple terms.\"}\n",
    "    ],\n",
    ")\n",
    "print(resp.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "385ba66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Short version: A vector database stores “meaning” as numbers so you can find similar things fast. It turns text, images, audio, or tables into vectors (lists of numbers). Similar items end up near each other in this high‑dimensional space. The database then finds nearest neighbors quickly, often in milliseconds, even among millions or billions of items.\n",
      "\n",
      "Why enterprises care\n",
      "- Make messy data searchable by meaning, not exact words (semantic search).\n",
      "- Power RAG (retrieve-augment-generate) for more accurate LLM answers.\n",
      "- Recommendations and personalization across products, content, or tickets.\n",
      "- Detect duplicates, near-duplicates, anomalies, or fraud patterns.\n",
      "- Cross‑modal search (e.g., search images with text).\n",
      "\n",
      "How it works (simple flow)\n",
      "1) Embed: An embedding model converts each item (document chunk, product, image) into a vector, typically 384–3072 dimensions.\n",
      "2) Store: Save the vector plus metadata (title, permissions, timestamps) as a record.\n",
      "3) Index: Build a specialized index for fast “nearest neighbor” search (ANN).\n",
      "4) Query: Convert the user query into a vector and search for nearest neighbors.\n",
      "5) Filter and re-rank: Apply metadata filters (tenant, region, date) and optionally re-rank with a stronger model or an LLM before using results.\n",
      "\n",
      "How it’s different from a traditional database or keyword search\n",
      "- Finds by meaning, not exact tokens. “How do I reset my laptop?” matches “factory restore procedure.”\n",
      "- Uses approximate nearest neighbor (ANN) indexes (like HNSW, IVF, PQ) to trade tiny accuracy for big speed.\n",
      "- Optimized for vector similarity (cosine, dot product, L2) plus metadata filters; less about joins and transactions.\n",
      "\n",
      "Core pieces you’ll choose\n",
      "- Embedding model: General vs domain-specific, multilingual needs, model versioning. Keep versions stable; re-embed if you switch models.\n",
      "- Index type:\n",
      "  - HNSW: great recall, low-latency, supports online inserts; more memory.\n",
      "  - IVF/IVF+PQ: scalable with compression; needs training step; good for very large datasets.\n",
      "  - GPU (FAISS/RAFT): very fast at high scale; consider cost and ops.\n",
      "- Distance metric: cosine or dot product for text; L2 for some vision/time-series.\n",
      "- Metadata filtering: Attribute filters (tenant, ACLs, type). Consider hybrid search (lexical + vector) for precision.\n",
      "- Consistency and updates: Freshness requirements (seconds vs minutes), delete semantics (“right to be forgotten”) and background compaction.\n",
      "- Security and governance: Row/attribute-level access, tenant isolation, encryption, audit, data residency, PII handling.\n",
      "- Cost and sizing: Memory/disk footprint, compression, throughput requirements.\n",
      "\n",
      "RAG in practice (typical enterprise pattern)\n",
      "- Chunk content (e.g., 200–500 tokens, overlap 10–20%), embed each chunk, store vector + source + ACLs.\n",
      "- At query time, embed the question, do vector search with filters, optionally re-rank with a cross-encoder, then send the top passages to the LLM with citations.\n",
      "- Cache frequent queries and results; monitor answer quality and hallucinations.\n",
      "\n",
      "Rules of thumb for sizing and performance\n",
      "- Raw vector size ≈ dimensions × 4 bytes (float32). Example: 1536-d ≈ 6 KB per vector. Index overhead can be 1.5–3× for HNSW.\n",
      "- 10 million vectors at 6 KB raw ≈ 60 GB raw; with HNSW overhead, 100–180 GB memory. Use PQ or scalar quantization to reduce 4–10× with some recall loss.\n",
      "- Latency: Single-digit to low tens of ms for top‑k=10 at 1–10M scale on a few CPU/GPU nodes; p95 depends on filters and network.\n",
      "- Quality: Target recall@k ≥ 0.9 for your use case. Track nDCG and business metrics (clicks, resolution time).\n",
      "\n",
      "Operational concerns\n",
      "- Ingestion: Batch vs streaming; dedup; document splitting; language detection; PII redaction.\n",
      "- Index maintenance: Background builds, compaction, snapshot/restore, re-index when model changes.\n",
      "- Deletions and “right to be forgotten”: Ensure tombstones propagate and data is actually purged from indexes and backups per policy.\n",
      "- Monitoring: Index size, memory, CPU/GPU, p50/p95 latency, QPS, recall, filter selectivity, error rates.\n",
      "- Multi-tenancy: Hard partitioning per tenant for strong isolation, or soft isolation with strict filters and ABAC; beware filter-first vs score-then-filter pitfalls.\n",
      "- Hybrid search: Combine BM25 (keywords) and vectors; re-rank top N with a cross-encoder for precision-critical queries (legal, healthcare).\n",
      "\n",
      "Where to run it\n",
      "- Dedicated vector databases (e.g., commercial managed services and open-source engines) for large scale and low latency with rich filtering.\n",
      "- Search engines with vector support (e.g., enterprise search platforms) for strong text search + semantic hybrid.\n",
      "- Relational DBs with vector extensions (e.g., PostgreSQL with pgvector, cloud variants) for moderate scale and simpler ops when you already live in SQL.\n",
      "- Choose managed if you want SLAs and reduced ops; self-host if data gravity, cost, or control dominate.\n",
      "\n",
      "When to use vs not use\n",
      "- Use when: you need semantic search across large, diverse data; recommendations; RAG; anomaly/duplicate detection.\n",
      "- Avoid or complement with keyword/SQL when: corpus is tiny; you require exact matching and complex joins; precision must be perfect; heavy transactional semantics are needed.\n",
      "\n",
      "Common pitfalls and how to avoid them\n",
      "- Embedding drift: Changing models silently degrades search. Version vectors and re-embed in batches with A/B validation.\n",
      "- Poor chunking: Too big hurts recall; too small loses context. Experiment and measure.\n",
      "- Over-filtering: Aggressive ACL/date filters can kill recall; pre-partition or build per-tenant shards to keep candidate sets large.\n",
      "- Data leakage: Enforce row-level security in the vector layer and in any cache; never bypass filters during re-ranking or LLM calls.\n",
      "- Multilingual mismatch: Use multilingual embeddings or per-language indexes; detect language at ingest.\n",
      "\n",
      "Quick glossary\n",
      "- Embedding: Numeric representation of meaning.\n",
      "- Vector: The list of numbers output by the embedding model.\n",
      "- ANN index: Data structure for fast approximate nearest neighbor search (HNSW, IVF, PQ).\n",
      "- Recall: How often the true nearest neighbors are found; higher is better.\n",
      "- Hybrid search: Combining lexical and vector scores.\n",
      "\n",
      "If you share rough data size, latency, and security requirements, I can suggest a concrete architecture and index choice.\n"
     ]
    }
   ],
   "source": [
    "#GPT-5 (Responses API; use your current gpt-5 model name)\n",
    "resp = client.responses.create(\n",
    "    model=\"gpt-5\",  # replace with the exact gpt-5* model on your account\n",
    "    input=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a precise technical explainer. Verbosity: medium.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Explain vector databases for enterprise in simple terms.\"}\n",
    "    ]\n",
    ")\n",
    "print(resp.output_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5089eb3a",
   "metadata": {},
   "source": [
    "Notes: Chat Completions uses messages=[...]. \n",
    "\n",
    "The Responses API accepts an input=[...] \n",
    "\n",
    "#array with the same roles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681ef3aa",
   "metadata": {},
   "source": [
    "2) Few-shot style transfer (show, then ask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e2c28ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could you please take a moment to fix the data pipeline? Thanks!\n"
     ]
    }
   ],
   "source": [
    "#GPT-4\n",
    "messages = [\n",
    "  {\"role\": \"system\", \"content\": \"You are a writing coach.\"},\n",
    "  {\"role\": \"user\", \"content\": \"Rewrite in a friendlier tone: 'Submit the report by EOD.'\"},\n",
    "  {\"role\": \"assistant\", \"content\": \"Could you please send the report by the end of the day? Thanks!\"},\n",
    "  {\"role\": \"user\", \"content\": \"Rewrite in the same friendly tone: 'Fix the data pipeline now.'\"}\n",
    "]\n",
    "\n",
    "resp = client.chat.completions.create(model=\"gpt-4o\", temperature=0.7, messages=messages)\n",
    "\n",
    "print(resp.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b03a4f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could you please fix the data pipeline as soon as you can? Thanks!\n"
     ]
    }
   ],
   "source": [
    "#GPT-5\n",
    "resp = client.responses.create(\n",
    "  model=\"gpt-5\",\n",
    "  input=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a writing coach. Match the tone of assistant examples. Verbosity: low.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Rewrite in a friendlier tone: 'Submit the report by EOD.'\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Could you please send the report by the end of the day? Thanks!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Rewrite in the same friendly tone: 'Fix the data pipeline now.'\"}\n",
    "  ]\n",
    ")\n",
    "print(resp.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cbf071",
   "metadata": {},
   "source": [
    "3) “Verbosity” / tone control (semantics, not just length)\n",
    "You can guide verbosity and tone declaratively in your system instruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "91a52666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Top-p Sampling (Nucleus Sampling):**\n",
      "- **Pros:** Dynamic control by adjusting samples, offers fine-tuned creativity by selecting from the most probable results.\n",
      "- **Cons:** May compromise coherence if 'p' value is too low, potentially omits diverse word options.\n",
      "\n",
      "**Temperature Sampling:**\n",
      "- **Pros:** Simple implementation; allows control over randomness by scaling probabilities, useful for consistent outputs.\n",
      "- **Cons:** High value can lead to incoherent results, while low value limits creativity by focusing on high-probability words.\n"
     ]
    }
   ],
   "source": [
    "#GPT-4\n",
    "resp = client.chat.completions.create(\n",
    "  model=\"gpt-4o\",\n",
    "  messages=[\n",
    "    {\"role\":\"system\",\"content\":\"Be concise (2–3 bullets). Tone: professional.\"},\n",
    "    {\"role\":\"user\",\"content\":\"Summarize the pros and cons of Top-p vs Temperature.\"}\n",
    "  ],\n",
    "  max_tokens=200\n",
    ")\n",
    "print(resp.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98103f98",
   "metadata": {},
   "source": [
    "Tip: In GPT-5, treat “verbosity” and “tone” as semantic dials expressed in instructions. (They complement, rather than replace, hard caps like max_output_tokens.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "477fc43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Temperature: Pros—smooth, single knob; preserves token ranking; works consistently across contexts. Cons—non-adaptive; high values admit very low-probability tokens (instability); low values can make outputs overconfident/repetitive.\n",
      "- Top-p (nucleus): Pros—adaptive to entropy; trims the unlikely tail, reducing bizarre tokens; maintains fluency in high-uncertainty settings. Cons—can cut out useful rare tokens; discontinuous control; in low-entropy prompts becomes near-greedy, reducing diversity.\n",
      "- Guidance: Prefer temperature for graded control; use top-p to bound “surprise.” Common combo: temperature ~0.7–1.0 with top-p ~0.9–0.95; avoid extremes and tune per task.\n"
     ]
    }
   ],
   "source": [
    "#GPT-5\n",
    "resp = client.responses.create(\n",
    "  model=\"gpt-5\",\n",
    "  input=[\n",
    "    {\"role\":\"system\",\"content\":\"Verbosity: low. Tone: professional. Output as 2–3 bullets.\"},\n",
    "    {\"role\":\"user\",\"content\":\"Summarize the pros and cons of Top-p vs Temperature.\"}\n",
    "  ]\n",
    ")\n",
    "print(resp.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075261af",
   "metadata": {},
   "source": [
    "5) Light reasoning with short rationale (no hidden chain-of-thought)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d3725027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For deterministic outputs, setting temperature=0 is better because it removes randomness by always choosing the highest probability option. In contrast, top_p=1 allows for sampling from the entire probability distribution, which can introduce variability.\n"
     ]
    }
   ],
   "source": [
    "#GPT-4\n",
    "resp = client.chat.completions.create(\n",
    "  model=\"gpt-4o\",\n",
    "  temperature=0.2,\n",
    "  messages=[\n",
    "    {\"role\":\"system\",\"content\":\"Answer with a brief rationale (2 sentences max).\"},\n",
    "    {\"role\":\"user\",\"content\":\"Which is better for deterministic outputs: temperature=0 or top_p=1?\"}\n",
    "  ]\n",
    ")\n",
    "print(resp.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4ad7e4df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature=0. It forces greedy/argmax decoding (no sampling), while top_p=1 only disables nucleus filtering and can still be stochastic; for strict determinism also avoid sampling and fix seeds/hardware settings.\n"
     ]
    }
   ],
   "source": [
    "#GPT-5\n",
    "resp = client.responses.create(\n",
    "  model=\"gpt-5\",\n",
    "  input=[\n",
    "    {\"role\":\"system\",\"content\":\"Provide the answer and a brief rationale (≤2 sentences). Verbosity: low.\"},\n",
    "    {\"role\":\"user\",\"content\":\"Which is better for deterministic outputs: temperature=0 or top_p=1?\"}\n",
    "  ]\n",
    ")\n",
    "print(resp.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4238fcf7",
   "metadata": {},
   "source": [
    "1. Zero-Shot Prompting\n",
    "No examples, just instructions.\n",
    "Best for straightforward tasks.\n",
    "Use Case (Developer – SQL query generation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab97621e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT *\n",
      "FROM Employee\n",
      "WHERE Salary > 100000;\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-5\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\":\n",
    "         \"Write a SQL query to fetch all employees with salary greater than 1,00,000 from the Employee table.\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29ac643",
   "metadata": {},
   "source": [
    "GPT-5 directly generates the query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20dc1912",
   "metadata": {},
   "source": [
    "2. Few-Shot Prompting\n",
    "Provide examples so the model learns the pattern.\n",
    "👉 Great for structured outputs (status reports, Jira updates, etc.).\n",
    "Use Case (Team Leader – Status updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b960cb27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: Implement user profile dashboard → Status: In Progress, expected by EOW.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Example 1:\n",
    "Task: Fix login bug → Status: Completed, merged to main branch.\n",
    "\n",
    "Example 2:\n",
    "Task: Update API documentation → Status: In Progress, expected by EOD.\n",
    "\n",
    "Now, generate status for:\n",
    "Task: Implement user profile dashboard.\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-5\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a82dfc",
   "metadata": {},
   "source": [
    "Chain of Thought (CoT) Prompting\n",
    "Ask GPT-5 to “think step by step” before answering.\n",
    "👉 Ideal for estimations, planning, or reasoning-heavy tasks.\n",
    "Use Case (Project Manager – Effort estimation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dd8f7bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here’s a structured way to think about effort, with clear assumptions, scope, and estimates. If you share your stack and constraints, I can refine to a tighter number.\n",
      "\n",
      "Assumptions\n",
      "- Platforms: Web app with existing auth (email+password), backend API, relational DB.\n",
      "- Email provider available (e.g., SendGrid/SES). If not, add time.\n",
      "- No existing MFA/SSO interaction to handle (not enforced at reset).\n",
      "- Security baseline: Argon2/bcrypt hashing, HTTPS, environment secrets managed.\n",
      "- You want a production-ready flow (no user enumeration, rate limits, logs).\n",
      "\n",
      "User flows\n",
      "- Request reset:\n",
      "  - User enters email. Response is always generic (“If this email exists…”).\n",
      "  - System creates one-time, time-limited token and emails a link.\n",
      "- Reset password:\n",
      "  - User opens link, sets new password matching policy.\n",
      "  - System validates token (single use, not expired), updates password, invalidates token, revokes active sessions, logs event, and sends confirmation email.\n",
      "\n",
      "Functional scope\n",
      "- Frontend\n",
      "  - Screens: Forgot Password, Check Your Email, Reset Password, Success.\n",
      "  - Form validation, password policy hints, accessibility, i18n hooks.\n",
      "- Backend\n",
      "  - Endpoints: POST /auth/forgot-password, POST /auth/reset-password.\n",
      "  - Token model: store hashed token, expiry, one-time use; link contains opaque token.\n",
      "  - Password update + session revocation.\n",
      "  - Emails: reset link; post-change notification.\n",
      "  - Rate limiting and abuse protection.\n",
      "- Security hardening\n",
      "  - No user enumeration.\n",
      "  - Hash reset tokens at rest; short TTL (e.g., 20–30 min).\n",
      "  - One-time use; invalidate on use and on password change.\n",
      "  - IP/device logging; optional CAPTCHA after N attempts.\n",
      "  - Strong password policy; breach password check (optional).\n",
      "- Ops/Monitoring\n",
      "  - Observability: metrics for requests, sends, conversions, errors.\n",
      "  - Audit logs for reset requested/used/failed.\n",
      "  - Secrets/config: email sender, token TTL, frontend base URL.\n",
      "- QA\n",
      "  - Test cases: happy paths, invalid/expired tokens, multiple requests, rate limits, accessibility, mobile viewport, email rendering (light/dark), international characters in email, session revocation, regression on login.\n",
      "\n",
      "Implementation plan (high level)\n",
      "1) Design UX and copy; define password policy and email templates.\n",
      "2) Create password_reset_tokens table (user_id, token_hash, expires_at, used_at, created_ip/ua).\n",
      "3) Implement /forgot-password:\n",
      "   - Always 200 with generic response.\n",
      "   - Create token; send email; rate-limit by IP and by identifier.\n",
      "4) Implement /reset-password:\n",
      "   - Validate token; enforce password policy; update hash; revoke sessions; log; notify.\n",
      "5) Frontend pages/forms with validation and error states.\n",
      "6) Security hardening: token hashing, one-time use, rate limiting, CAPTCHA toggle.\n",
      "7) Monitoring, logs, alerts; redaction of PII in logs.\n",
      "8) QA, cross-browser, email client testing; fix; deploy.\n",
      "\n",
      "Effort estimate (ranges assume 1 engineer + QA; business day is 6–7 focused dev hours)\n",
      "- MVP (basic, web-only, minimal hardening)\n",
      "  - Backend core (tokens, endpoints, email send): 10–16 hours\n",
      "  - Frontend pages and validation: 8–12 hours\n",
      "  - Basic logging and tests: 4–6 hours\n",
      "  - QA + fixes: 6–10 hours\n",
      "  - Total: 28–44 hours (about 4–6 dev days)\n",
      "\n",
      "- Production-ready (recommended)\n",
      "  - Backend core: 12–18 hours\n",
      "  - Frontend (accessibility, polished states): 10–14 hours\n",
      "  - Email templates (responsive), post-change notification: 4–6 hours\n",
      "  - Rate limiting, CAPTCHA hook, token hashing at rest, session revocation: 8–12 hours\n",
      "  - Monitoring/metrics/audit logs: 4–8 hours\n",
      "  - QA (test matrix, email clients), security review, fixes: 12–18 hours\n",
      "  - DevOps/config, secrets, feature flags: 4–6 hours\n",
      "  - Total: 54–82 hours (about 1.5–2.5 weeks)\n",
      "\n",
      "- Enterprise-grade (extras)\n",
      "  - MFA-aware resets (e.g., require 2FA on reset): 8–16 hours\n",
      "  - Password history and breach checks: 8–12 hours\n",
      "  - Admin tooling (trigger reset, view audit, throttling dashboard): 10–16 hours\n",
      "  - Localization of UI and emails (2–3 languages): 6–12 hours\n",
      "  - Expanded compliance logging and alerts: 6–10 hours\n",
      "  - Additional QA/security testing (abuse, perf): 10–16 hours\n",
      "  - Incremental total over production: +48–82 hours\n",
      "  - Overall total: ~102–164 hours (3–5 weeks)\n",
      "\n",
      "Dependencies and risks that can move the estimate\n",
      "- No existing email provider/infrastructure: add 8–12 hours for integration and domain/auth (SPF/DKIM/DMARC) setup.\n",
      "- Mobile apps (native): add 8–16 hours per platform for UI and deep linking.\n",
      "- Legacy auth/session model complexity: +6–12 hours to ensure safe revocation.\n",
      "- High-abuse environment requiring aggressive bot controls: +8–16 hours.\n",
      "\n",
      "Acceptance criteria (condensed)\n",
      "- Generic responses avoid email enumeration; abuse protections in place.\n",
      "- Tokens are single-use, short-lived, stored hashed; logs don’t leak secrets.\n",
      "- Password change invalidates old sessions and triggers confirmation email.\n",
      "- UX is accessible and works on mobile/desktop; emails render across major clients.\n",
      "- Metrics and audit events exist for request, sent, opened (if available), reset success/failure.\n",
      "\n",
      "Questions to tighten the estimate\n",
      "- Tech stack (frontend/back end), session model (JWT vs server sessions), and DB?\n",
      "- Existing email provider and templates?\n",
      "- Need for CAPTCHA, rate limits, and breach checks at launch?\n",
      "- Mobile apps involved? MFA/SSO in scope?\n",
      "- Localization requirements?\n",
      "\n",
      "If your needs match “production-ready” on web with an existing email provider, plan for roughly 1.5–2.5 weeks for one engineer plus QA.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Estimate effort for building a feature where customers can reset their password.\n",
    "Think step by step before giving the final answer.\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-5\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2045524",
   "metadata": {},
   "source": [
    "GPT-5 outputs structured reasoning (API, frontend, testing, deployment) + final 6 days estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1631148",
   "metadata": {},
   "source": [
    "4. ReAct (Reason + Act) Prompting\n",
    "Combine reasoning with tool usage.\n",
    "👉 Used in Agentic AI systems where GPT interacts with APIs, databases, or tools.\n",
    "Use Case (Developer – Jira + GitHub integration):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84d5bd65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought → I can’t share my internal chain-of-thought, and I don’t have access to your Jira/GitHub data yet.\n",
      "\n",
      "Action → Please provide:\n",
      "- The Jira issue URL or key for “Payment Gateway Timeout Bug” (e.g., PAY-123)\n",
      "- The GitHub repository (owner/repo) or confirm the Jira issue has the Development panel with linked commits\n",
      "If access is private, you can paste the issue’s current status and the most recent linked commit message here.\n",
      "\n",
      "Observation → Awaiting the Jira/GitHub details to proceed.\n",
      "\n",
      "Final Answer → Once I have the issue/repo info, I’ll check whether the Jira issue is resolved and, if it is, I’ll summarize the latest linked GitHub commit message.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "You are a project assistant.\n",
    "Task: Find out if the 'Payment Gateway Timeout Bug' in Jira is resolved.\n",
    "If yes, summarize the latest GitHub commit message linked to it.\n",
    "\n",
    "Follow this format:\n",
    "Thought → Action → Observation → Final Answer\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-5\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0731d0",
   "metadata": {},
   "source": [
    "GPT-5 simulates reasoning:\n",
    "•\tThought: “Check Jira ticket…”\n",
    "•\tAction: Call Jira API (in real agent setup).\n",
    "•\tObservation: “Bug marked resolved.”\n",
    "•\tFinal Answer: “Commit message: Fixed payment timeout by adding retry logic.”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bcd449",
   "metadata": {},
   "source": [
    "Key GPT-5 Parameters\n",
    "1. temperature\n",
    "Controls creativity vs. determinism.\n",
    "Range: 0 → 2\n",
    "0 = deterministic (same answer each time, safe for coding/testing).\n",
    "1 = balanced (default, natural variety).\n",
    ">1 = very creative (good for brainstorming).\n",
    "Example (Creative vs. Precise Response):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6447b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "prompt = \"Write a tagline for an AI-powered banking app.\"\n",
    "\n",
    "# Deterministic (temperature=0)\n",
    "response1 = client.chat.completions.create(\n",
    "    model=\"gpt-5\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# Creative (temperature=1.2)\n",
    "response2 = client.chat.completions.create(\n",
    "    model=\"gpt-5\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    temperature=1.2\n",
    ")\n",
    "\n",
    "print(\"Deterministic:\", response1.choices[0].message.content)\n",
    "\n",
    "print(\"Creative:\", response2.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e98da08",
   "metadata": {},
   "source": [
    "✅ Team Leaders will see how low temp = consistent, high temp = more variety."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1330f9",
   "metadata": {},
   "source": [
    "2. max_tokens\n",
    "•\tControls how long the response can be.\n",
    "•\tUseful to prevent very long answers.\n",
    "•\tThink of tokens ≈ words/parts of words (1 token ≈ 4 chars in English).\n",
    "Example (Summary with limited words):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55579a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Summarize the Agile methodology in simple terms.\"\n",
    "\n",
    "# Short summary\n",
    "response1 = client.chat.completions.create(\n",
    "    model=\"gpt-5\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    max_tokens=50   # short\n",
    ")\n",
    "\n",
    "# Detailed summary\n",
    "response2 = client.chat.completions.create(\n",
    "    model=\"gpt-5\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    max_tokens=200  # longer\n",
    ")\n",
    "\n",
    "print(\"Short:\", response1.choices[0].message.content)\n",
    "\n",
    "print(\"Detailed:\", response2.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74575ee",
   "metadata": {},
   "source": [
    "3. stop sequences\n",
    "•\tDefines where the model should stop generating.\n",
    "•\tUseful to enforce formatting (tables, JSON, dialogues).\n",
    "•\tExample: stop when model outputs \"\\nHuman:\" so it doesn’t continue a dialogue.\n",
    "Example (Controlled Dialogue):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ae7f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Human: Hello, who are you?\n",
    "AI: I am an AI assistant.\n",
    "Human: Tell me about DevOps.\n",
    "AI:\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-5\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    stop=[\"Human:\"],  # stop when \"Human:\" appears\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14adbdd2",
   "metadata": {},
   "source": [
    "🌟 What is Function Calling?\n",
    "•\tFunction calling allows GPT-5 to decide when and how to call external functions/tools.\n",
    "•\tUseful for:\n",
    "•\tFetching live data (APIs, DB queries).\n",
    "•\tEnforcing structured outputs (JSON).\n",
    "•\tAgentic AI workflows (Reason + Act).\n",
    "Think of GPT-5 as the brain, and functions/tools as the hands.\n",
    " \n",
    "1. Basic Function Calling\n",
    "We define a function (say, get_weather) and let GPT-5 decide when to call it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba87207",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "# Define a tool (function schema)\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_weather\",\n",
    "            \"description\": \"Get the current weather in a city\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"city\": {\"type\": \"string\", \"description\": \"City name\"}\n",
    "                },\n",
    "                \"required\": [\"city\"]\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Ask GPT-5\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-5\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"What’s the weather in Pune?\"}],\n",
    "    tools=tools\n",
    ")\n",
    "\n",
    "print(response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9c24e9",
   "metadata": {},
   "source": [
    " Developers then execute this function (via your Python code/API), fetch real data, and pass results back to GPT-5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b58cc7e",
   "metadata": {},
   "source": [
    "2. Function Execution Loop (Tool Usage)\n",
    "Once GPT-5 asks for a function call, you execute it, then send results back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3b2ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated external function\n",
    "def get_weather(city):\n",
    "    return {\"city\": city, \"temp\": \"28°C\", \"condition\": \"Sunny\"}\n",
    "\n",
    "# Step 1: GPT decides function call\n",
    "first = client.chat.completions.create(\n",
    "    model=\"gpt-5\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"What’s the weather in Pune?\"}],\n",
    "    tools=tools\n",
    ")\n",
    "\n",
    "tool_call = first.choices[0].message.tool_calls[0]\n",
    "city_name = tool_call.function.arguments['city']\n",
    "\n",
    "# Step 2: Execute the tool\n",
    "result = get_weather(city_name)\n",
    "\n",
    "# Step 3: Send result back to GPT\n",
    "second = client.chat.completions.create(\n",
    "    model=\"gpt-5\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"What’s the weather in Pune?\"},\n",
    "        {\"role\": \"assistant\", \"tool_calls\": first.choices[0].message.tool_calls},\n",
    "        {\"role\": \"tool\", \"tool_call_id\": tool_call.id, \"content\": str(result)}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(second.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4590711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Report:\n",
      "- Task: Design database schema for user profiles\n",
      "- Progress: Schema created\n",
      "- Blockers: None\n",
      "- Next Steps: Run migrations, test, push to repository\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "prompt = \"\"\"\n",
    "You are a Team Leader assistant.\n",
    "You generate concise, structured project status updates for developers.\n",
    "Format the output in this template:\n",
    "\n",
    "Status Report:\n",
    "- Task:\n",
    "- Progress:\n",
    "- Blockers:\n",
    "- Next Steps:\n",
    "\n",
    "Examples:\n",
    "\n",
    "Developer: \"I worked on the login API, added JWT authentication, but still debugging token expiry issue. Tomorrow I’ll fix that and write unit tests.\"\n",
    "Status Report:\n",
    "- Task: Implement login API with JWT auth\n",
    "- Progress: Completed core implementation, added authentication\n",
    "- Blockers: Token expiry issue under debugging\n",
    "- Next Steps: Fix expiry issue, write unit tests\n",
    "\n",
    "Developer: \"I integrated the payment gateway, sandbox works fine. Deployment pending security review.\"\n",
    "Status Report:\n",
    "- Task: Payment gateway integration\n",
    "- Progress: Sandbox integration successful\n",
    "- Blockers: Awaiting security review\n",
    "- Next Steps: Proceed with deployment after review\n",
    "\n",
    "Now process the following update:\n",
    "\n",
    "Developer: \"I created database schema for user profiles, migrations are pending. Tomorrow will test and push to repo.\"\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-5\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
